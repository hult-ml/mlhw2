{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA1H2AHyVWQ"
   },
   "source": [
    "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
    "\n",
    "# Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQSvK4tZ3ias"
   },
   "source": [
    "## Table of Contents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llCqCNUTz7wG"
   },
   "source": [
    "* **HW-2: Rossman Kaggle: Forecasting Sales**\n",
    "  * Instructions\n",
    "  * Learning Goals\n",
    "  * Loading the DataFrame\n",
    "  * Q1: Data-Preprocesing and Understanding the data **(10 marks)**(HW1_Part1)\n",
    "  * Q2: Modelling without Entity Embeddings**(30 marks)**(HW1_Part2)\n",
    "    * 2.1 Baseline model - Linear Regression  \n",
    "    * 2.2 Random Forest \n",
    "    * 2.3 XGBoost \n",
    "    * 2.4 Multi Layer Perceptron \n",
    "  * Q3: Modelling MLP with Entity Embeddings**(10 marks)**(HW1_Part3)\n",
    "  * Q4 : Modelling other models with Entity Embeddings **(40 marks)**(HW1_Part4)\n",
    "    * 4.1 Baseline model - Linear Regression  \n",
    "    * 4.2 Random Forest \n",
    "    * 4.3 XGBoost\n",
    "  * Q4: Final Comments **(10 marks)** (HW1_Part4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxkV2910pFEw"
   },
   "source": [
    "## Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhwik43rpHj_"
   },
   "source": [
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "- Running cells out of order is a common pitfall in Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
    "\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook.\n",
    "\n",
    "- \n",
    "- Comment your code well.\n",
    "\n",
    "- \n",
    "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
    "\n",
    "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
    "\n",
    "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0xOsPAh0NI7"
   },
   "source": [
    "## Learning Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWGfd3i0QI6"
   },
   "source": [
    "**We will look here into the practicalities of Trees, MLPs and Entity Embedding.**\n",
    "\n",
    "The homework is divided into four main parts:\n",
    "1. Data-preprocessing\n",
    "2. Developing different models and evaulating the models - without Entity Embeddings\n",
    "3. Pass on the entity embeddings from Neural Network model to other models and evaluate the models\n",
    "4. Compare the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sed_rlx_pU3s"
   },
   "source": [
    "## Read this first!\n",
    "\n",
    "The homework is divided into **4 notebooks**\n",
    "1. Preprocessing and Storing Data\n",
    "2. Modelling without Entity Embeddings\n",
    "3. MultiLayer Perceptron with Entity Embeddings \n",
    "4. Modelling with Entity Embeddings and Comparing the results\n",
    "\n",
    "\n",
    "This Homework is based on the **paper attached in the data folder**\n",
    "\n",
    "Lets talk about the paper first:\n",
    "\n",
    "A very simple explaination of what the paper is trying to achieve is to show how to accuracy of the model changes using Entity Embeddings. \n",
    "\n",
    "Things to note:\n",
    "\n",
    "1. We want the results to be like the results shown in the paper\n",
    "\n",
    "**We will not be implementing KNNs - instead just for our comparison we will use Linear regression**\n",
    "\n",
    "2. The paper specifically mentions the parameters it uses to achieve these results, and we will be using the same as well. \n",
    "\n",
    "\n",
    "**Again remember we will not be implementing KNNs - instead just for our comparison we will use Linear regression**\n",
    "\n",
    "\n",
    "3. The last point we want you to note is the following: we will be using MAPE as the metric just like the paper does.\n",
    "\n",
    "\n",
    "#### So lets get started! Please note: this particular notebook is only for Data Preprocessing and saving the datafile. The notebooks for Modelling without Entity Embeddings MLP with Entity Embedding and other models with Entity Embeddings is Part2 and Part3 and Part4. \n",
    "\n",
    "**Why are we doing this?** \n",
    "\n",
    "Each of this processing requires high RAM, which you may or may not have access to - hence we split the work in four parts and call the work from each part into the next one! Also this helps us modularise it better!!\n",
    "\n",
    "Since we have not done entity embeddings the code there is provided for you!!!!\n",
    "\n",
    "Learn from it!! It goes from the session we had on e,beddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Ch786JHpRvl"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import xgboost as xgb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RRf1faokxPi"
   },
   "source": [
    "## Q1. Data Pre-Processing and Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPoDLSZOkxPi"
   },
   "source": [
    "### 1.1 Loading and understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdAE5YJCkxPj"
   },
   "source": [
    "#### About the data\n",
    "\n",
    "Most of the fields are self-explanatory. The following are descriptions for those that aren't. \n",
    "\n",
    "1. **Id** - an Id that represents a (Store, Date) duple within the test set\n",
    "2. **Store** - a unique Id for each store\n",
    "3. **Sales** - the turnover for any given day (this is what you are predicting)\n",
    "4. **Customers** - the number of customers on a given day\n",
    "5. **Open** - an indicator for whether the store was open: \n",
    "    * 0 = closed\n",
    "    * 1 = open\n",
    "6. **StateHoliday** - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. \n",
    "    * a = public holiday \n",
    "    * b = Easter holiday\n",
    "    * c = Christmas\n",
    "    * 0 = None\n",
    "7. **SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "8. **StoreType** - differentiates between 4 different store models: a, b, c, d\n",
    "9. **Assortment** - describes an assortment level: \n",
    "    * a = basic\n",
    "    * b = extra\n",
    "    * c = extended\n",
    "10. **CompetitionDistance** - distance in meters to the nearest competitor store\n",
    "11. **CompetitionOpenSince[Month/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n",
    "12. **Promo** - indicates whether a store is running a promo on that day\n",
    "13. **Promo2** - Promo2 is a continuing and consecutive promotion for some stores: \n",
    "    * 0 = store is not participating\n",
    "    * 1 = store is participating\n",
    "14. **Promo2Since[Year/Week]** - describes the year and calendar week when the store started participating in Promo2\n",
    "15. **PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81YPHFtmkxPj"
   },
   "source": [
    "**Note, since this data is large, we do not want to convert this data into dataframes, we will store it as array of dictionaries and pass the same to the models.**\n",
    "**Also, we reccommend using Google Colab for completing this Homework.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWNvvXvduWrV"
   },
   "outputs": [],
   "source": [
    "#importing the data as a string \n",
    "#your code here \n",
    "train_set = (\"________\")\n",
    "stores_set = (\"________\")\n",
    "store_states = (\"________\")\n",
    "test_set = (\"________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtgypPZUDZdI"
   },
   "source": [
    "We will now define functions:\n",
    "1. To convert our csv files into dictionaries\n",
    "2. To replace nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9udTMgNsxk7g"
   },
   "outputs": [],
   "source": [
    "def csv2dicts(csvfile):\n",
    "    data = []\n",
    "    keys = []\n",
    "    for row_index, row in enumerate(csvfile):\n",
    "        if row_index == 0:\n",
    "            keys = row\n",
    "            print(row)\n",
    "            continue\n",
    "        data.append({key: value for key, value in zip(keys, row)})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5FBsI4Zxm8_"
   },
   "outputs": [],
   "source": [
    "def set_nan_as_string(data, replace_str='0'):\n",
    "    for i, x in enumerate(data):\n",
    "        for key, value in x.items():\n",
    "            if value == '':\n",
    "                x[key] = replace_str\n",
    "        data[i] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txMeck53u6Rf"
   },
   "source": [
    "Save the train_set as a dictionary using csv2dicts function defined above. \n",
    "\n",
    "Further save this as a pickle file - call it **train_set.pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhynS2EgvDtw"
   },
   "outputs": [],
   "source": [
    "# save the train_set as a dictionary using csv2dicts function defined above. \n",
    "# Save this as a pickle file - call it train_set.pickle\n",
    "#your code here\n",
    "with open(train_set) as csvfile:\n",
    "    data = csv.reader(csvfile, delimiter=',')\n",
    "    with open('train_set.pickle', 'wb') as f:\n",
    "        #your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Byj46QzvfwB"
   },
   "source": [
    "If you look at store_states - it is basically sharing information about which stores are located in which states. Hence we will add this in the stores_set itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVHbmWIVvKOY"
   },
   "outputs": [],
   "source": [
    "#lets do the same thing for the store_set and store_states - call this pickle as store_set.pickle\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqZtSnaRv86g"
   },
   "source": [
    "Next we want to store the train_data length, hence load the data back from the pickle files saved and only assign num_records as the length of the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1X2PVJRivi0Y"
   },
   "outputs": [],
   "source": [
    "with open('train_set.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    num_records = len(train_data)\n",
    "with open('store_set.pickle', 'rb') as f:\n",
    "    store_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WuDe_H9wImc"
   },
   "source": [
    "If you have saved and loaded the files correctly then **train_data[1]** and **store_data[1]** should be as follows:\n",
    "\n",
    "![Mape.jpeg](https://drive.google.com/uc?export=view&id=1D7IMgfjbRvWNuJV_v5nx5H7TfzGjP811)\n",
    "\n",
    "\n",
    "Check if the column names are the same - if not recheck the previous codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YK8-EY45v-eI"
   },
   "outputs": [],
   "source": [
    "#check the same\n",
    "train_data[1], store_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYxIcQVHkW5Z"
   },
   "source": [
    "### 1.2 Feature list\n",
    "\n",
    "We will define a function to extract features from the data\n",
    "\n",
    "The function should return the following paramters:\n",
    "* the store index = from the train_set it should show the 'store'\n",
    "* year = this should come from train_set 'Date'\n",
    "* month = this should come from train_set 'Date'\n",
    "* day = this should come from train_set 'Date'\n",
    "* day_of_week = this should come from train_set 'DayOfWeek'\n",
    "* check if the store is open \n",
    "    * if yes - save that \n",
    "    * else it should save 1\n",
    "* promo = this should come from train_set 'Promo'\n",
    "* store_state = this should come from store_state 'State'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V94HyNa9YZpv"
   },
   "outputs": [],
   "source": [
    "def feature_list(record):\n",
    "    #your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_xfJP_UlI5x"
   },
   "source": [
    "Now lets create two dictionaries - train_data_X and train_data_y \n",
    "\n",
    "* Run through the train_set, and check if the 'Sales' are not equal to 0 and 'Open' is not equal to 0\n",
    "* If yes, then store the features(from feature list) into a variable named f1\n",
    "* append the f1 values in train_data_X\n",
    "* append the **Sales not equal** to 0 to train_data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hi0VVRT2Yon7"
   },
   "outputs": [],
   "source": [
    "train_data_X = []\n",
    "train_data_y = []\n",
    "\n",
    "for record in train_data:\n",
    "  #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZ0pwPL5xktL"
   },
   "outputs": [],
   "source": [
    "#again check how your train_data_X looks\n",
    "train_data_X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMq8Ebf2o8Vz"
   },
   "source": [
    "The next step is going to be labelencoding the train_data_X. We do this using LabelEncoder from sklearn\n",
    "\n",
    "We will run this for the complete train_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lapEjeLamUvd"
   },
   "outputs": [],
   "source": [
    "check_X = train_data_X\n",
    "check_X = np.array(check_X)\n",
    "train_data_X = np.array(train_data_X)\n",
    "les = []\n",
    "for i in range(train_data_X.shape[1]):\n",
    "    #your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaAS0GmYx7Je"
   },
   "outputs": [],
   "source": [
    "#again check how your train_data_X looks \n",
    "train_data_X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Ine8QPyWYL"
   },
   "source": [
    "We will dump the les dictionary(defined in the previous step) into les.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WIFRlmcszPMu"
   },
   "outputs": [],
   "source": [
    "with open('les.pickle', 'wb') as f:\n",
    "    #your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znP43J2PJjbO"
   },
   "source": [
    "And convert our train_data_X as int datatype, and save our train_data_y as an numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "u6rE23PHJeVn"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOx4-rgAy-U_"
   },
   "source": [
    "Finally we will store our train_data_X, train_data_y in a pickle file - **feature_train_data.pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vVr3ivy2zQ_v"
   },
   "outputs": [],
   "source": [
    "with open('feature_train_data.pickle', 'wb') as f:\n",
    "  #your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yg0ygwmq2agg"
   },
   "source": [
    "## You are done with Part 1 of the Homework!\n",
    "\n",
    "\n",
    "Save all the pickle files locally in your system/drive!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VxkV2910pFEw"
   ],
   "name": "HW2_Part1_distribute.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
