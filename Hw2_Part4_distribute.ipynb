{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA1H2AHyVWQ"
   },
   "source": [
    "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
    "# Part 4 : Modelling with embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jBFtovs3nh1"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlux5Gn-n-Gz"
   },
   "source": [
    "We will repeat the first initial steps again from Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Hi8ehR7WFX"
   },
   "source": [
    "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arl_aj6X7VQk"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwMxtFTUDRFN"
   },
   "outputs": [],
   "source": [
    "# we will split the train_ratio and 90% and 10% and set the train_size\n",
    "train_ratio = 0.9\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XpP1Emg2Dni0"
   },
   "outputs": [],
   "source": [
    "#lets look at our data\n",
    "X[1], y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2fhYw49Dz3w"
   },
   "source": [
    "The next set of inputs is following:\n",
    "\n",
    "1. Do you want to one hot encode the data?\n",
    "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
    "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
    "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tqtv6VWWDuZu"
   },
   "outputs": [],
   "source": [
    "one_hot_as_input = False #one_hot is set to false\n",
    "embeddings_as_input = True #embeddings later on needs to set to true\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ayLSo_1E0yF"
   },
   "source": [
    "## Now lets work with Models with Entity embedding!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTYK-isysXiu"
   },
   "source": [
    "Now that you have saved embeddings - push this into the other models as an input with X. \n",
    "\n",
    "How will we do this? \n",
    "\n",
    "We need to update our X values. \n",
    "\n",
    "1. We will define a function called embed_features, which will combine the embedding with X. \n",
    "2. Call this function and update it with the inital X values taken from the pickle file - features_train_data\n",
    "3. Then split you data, X_emb - into Xtrain and X_Val, y_train and y_val remain the same\n",
    "4. Sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbymoNn65ETi"
   },
   "outputs": [],
   "source": [
    "#call our saved embeddings from part 3\n",
    "saved_embeddings_fname = ___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUVh51hgqCyp"
   },
   "outputs": [],
   "source": [
    "#combining embedding\n",
    "def embed_features(X, saved_embeddings_fname):\n",
    "    f_embeddings = open(saved_embeddings_fname, \"rb\")\n",
    "    embeddings = pickle.load(f_embeddings)\n",
    "\n",
    "    index_embedding_mapping = {1: 0, 2: 1, 4: 2, 5: 3, 6: 4, 7: 5}\n",
    "    X_embedded = []\n",
    "\n",
    "    (num_records, num_features) = X.shape\n",
    "    for record in X:\n",
    "        embedded_features = []\n",
    "        for i, feat in enumerate(record):\n",
    "            feat = int(feat)\n",
    "            if i not in index_embedding_mapping.keys():\n",
    "                embedded_features += [feat]\n",
    "            else:\n",
    "                embedding_index = index_embedding_mapping[i]\n",
    "                embedded_features += embeddings[embedding_index][feat].tolist()\n",
    "\n",
    "        X_embedded.append(embedded_features)\n",
    "\n",
    "    return np.array(X_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf8U7znjHZrW"
   },
   "source": [
    "**Explain what is happening the function above**\n",
    "\n",
    "your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-wcLbyHps8_W"
   },
   "outputs": [],
   "source": [
    "#check if embedding is needed, if yes call embed_features - with X and the embeddings passed to it - call this new X as X_emb\n",
    "if embeddings_as_input:\n",
    "  #your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDzhYvCJHm8d"
   },
   "source": [
    "Split the train and validation based on train size and on the new X_emb values from the previous code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLTrTSGPEaGw"
   },
   "outputs": [],
   "source": [
    "#update the X_train and X_val\n",
    "#your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StnmulRkEnRt"
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = np.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ednNv14aEpjS"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1I6AT5MuRTH"
   },
   "source": [
    "## Add the embeddings into the Models and check their MAPE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmJYNA0PH5AU"
   },
   "source": [
    "All the models defined here will have the same parameters as the ones defined in Part 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdA5c3CMukyH"
   },
   "source": [
    "### Model 1: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTRegclpugs3"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4Sjp7NQu1ow"
   },
   "source": [
    "### Model 2: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGX-_sRYkxPv"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYvVmYMpu_Vi"
   },
   "source": [
    "### Model 3: Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1--s_4pGOuB"
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhM6IVJJv0rU"
   },
   "source": [
    "## Final Commments!\n",
    "\n",
    "Apart from how long this homework was, lets make some other final comments\n",
    "\n",
    "Question 1: Did you models with Entity Embeddings perfom better?\n",
    "\n",
    "Question 2: Now that you have completed this homework, lets answer the main purpose of the homework - How do you think entity embeddings improved the MAPE score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hw2_Part4_distribute.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
