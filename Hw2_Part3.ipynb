{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaA1H2AHyVWQ"
   },
   "source": [
    "# Homework 2: Rossman Kaggle: Forecasting Sales\n",
    "# Part 3 : Extracting embeddings!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6jBFtovs3nh1"
   },
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib import cm\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Activation, Reshape\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "import pickle\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlux5Gn-n-Gz"
   },
   "source": [
    "We will repeat the first initial steps again from Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5Hi8ehR7WFX"
   },
   "source": [
    "Lets import the feature_train_data.pickle file and set X,y values from the pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "arl_aj6X7VQk"
   },
   "outputs": [],
   "source": [
    "f = open('/content/drive/MyDrive/feature_train_data.pickle', 'rb')\n",
    "(X, y) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cwMxtFTUDRFN"
   },
   "outputs": [],
   "source": [
    "# we will split the train_ratio and 90% and 10% and set the train_size\n",
    "train_ratio = 0.9\n",
    "num_records = len(X)\n",
    "train_size = int(train_ratio * num_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpP1Emg2Dni0",
    "outputId": "e67b3f6c-671f-460c-bf64-5bb43424a811"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0, 1058,    1,    0,    0,    0,    0,    1]), 4491)"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at our data\n",
    "X[1], y[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2fhYw49Dz3w"
   },
   "source": [
    "The next set of inputs is following:\n",
    "\n",
    "1. Do you want to one hot encode the data?\n",
    "2. Do you want to provide embeddings as input - this will be set to True for models with entity embeddings\n",
    "3. Do you want to save the emmbeddings? - again set to true if you want to entity embeddings\n",
    "4. if 3 is set to true, we want to save them to a embeddings.pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Tqtv6VWWDuZu"
   },
   "outputs": [],
   "source": [
    "embeddings_as_input = True #embeddings later on needs to set to true\n",
    "save_embeddings = True\n",
    "saved_embeddings_fname = \"embeddings.pickle\"  # set save_embeddings to True to create this file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCk7tyVQEdy6"
   },
   "source": [
    "Split the data based on the train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hLTrTSGPEaGw"
   },
   "outputs": [],
   "source": [
    "X_train = X[:train_size]\n",
    "X_val = X[train_size:]\n",
    "y_train = y[:train_size]\n",
    "y_val = y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "StnmulRkEnRt"
   },
   "outputs": [],
   "source": [
    "def sample(X, y, n):\n",
    "  #your code here\n",
    "    '''random samples'''\n",
    "    num_row = X.shape[0]\n",
    "    indices = np.random.randint(num_row, size=n)\n",
    "    return X[indices, :], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ednNv14aEpjS",
    "outputId": "7d6cd123-d6f7-47ab-fd0a-35773d0d723c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples used for training: 200000\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = sample(X_train, y_train, 200000)  # Simulate data sparsity\n",
    "print(\"Number of samples used for training: \" + str(y_train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "itef2JvmQcwk"
   },
   "outputs": [],
   "source": [
    "X_val, y_val = sample(X_val, y_val, 200000)  # Simulate data sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ayLSo_1E0yF"
   },
   "source": [
    "## Now lets work with Models with Entity embedding!!\n",
    "\n",
    "To read about Entity Embeddings in more detail and its use cases look at the follwing links:\n",
    "1. [Enhancing categorical features with Entity Embeddings](https://towardsdatascience.com/enhancing-categorical-features-with-entity-embeddings-e6850a5e34ff)\n",
    "2. [Understanding Entity Embeddings and Itâ€™s Application](https://towardsdatascience.com/understanding-entity-embeddings-and-its-application-69e37ae1501d#:~:text=Loosely%20speaking%2C%20entity%20embedding%20is,a%20sentence%2C%20or%20a%20paragraph.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uOEqzd8BzK4"
   },
   "source": [
    "We will be creating the following embeddings:\n",
    "\n",
    "1. input_store - with input shape as (1,) \n",
    "  * The input_store will be passed to output_store to create an embedding layer with embedding shape as 1115, 10\n",
    "  * we will reshape this to target_shape=(10,)\n",
    "2. input_dow - with input shape as (1,) \n",
    "  * The input_dow will be passed to output_dow to create an embedding layer with embedding shape as 7, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "3. input_promo - with input shape as (1,) \n",
    "  * output promo will be a dense(1) for input_promo\n",
    "4. input_year - with input shape as (1,) \n",
    "  * The input_year will be passed to output_year to create an embedding layer with embedding shape as 3, 2\n",
    "  * we will reshape this to target_shape=(2,)\n",
    "5. input_month - with input shape as (1,) \n",
    "  * The input_month will be passed to output_month to create an embedding layer with embedding shape as 12, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "6. input_day - with input shape as (1,) \n",
    "  * The input_day will be passed to output_day to create an embedding layer with embedding shape as 31, 10\n",
    "  * we will reshape this to target_shape=(10,)\n",
    "7. input_germanstate - with input shape as (1,) \n",
    "  * The input_germanstate will be passed to output_germanstate to create an embedding layer with embedding shape as 12, 6\n",
    "  * we will reshape this to target_shape=(6,)\n",
    "\n",
    "\n",
    "* Construct an input model with all the inputs \n",
    "* Construct an output embeddings with all the outputs( basically the embeddings) - concatenate this and call it the **output model**\n",
    "\n",
    "Set the output model with the following parameters:\n",
    "\n",
    "1. Dense Layer - 1000 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
    "2. Dense Layer - 500 neurons, keep the kernel_initializer as uniform, with activaation as relu\n",
    "3. Final dense layer with 1 neuron, and activation as sigmoid\n",
    "4. Create a KerasModel called modelNN_emb:\n",
    "\n",
    "       modelNN_emb = KerasModel(inputs=input_model, outputs=output_model)\n",
    "4. Compile the model on mean absolute error and optimizer as adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "0qGSxshWqRCu"
   },
   "outputs": [],
   "source": [
    "#Define the embedding NN model\n",
    "#your code here\n",
    "input_store = Input(shape=(1,))\n",
    "output_store = Embedding(1115, 10, name='store_embedding')(input_store)\n",
    "output_store = Reshape(target_shape=(10,))(output_store)\n",
    "\n",
    "input_dow = Input(shape=(1,))\n",
    "output_dow = Embedding(7, 6, name='dow_embedding')(input_dow)\n",
    "output_dow = Reshape(target_shape=(6,))(output_dow)\n",
    "\n",
    "input_promo = Input(shape=(1,))\n",
    "output_promo = Dense(1)(input_promo)\n",
    "\n",
    "input_year = Input(shape=(1,))\n",
    "output_year = Embedding(3, 2, name='year_embedding')(input_year)\n",
    "output_year = Reshape(target_shape=(2,))(output_year)\n",
    "\n",
    "input_month = Input(shape=(1,))\n",
    "output_month = Embedding(12, 6, name='month_embedding')(input_month)\n",
    "output_month = Reshape(target_shape=(6,))(output_month)\n",
    "\n",
    "input_day = Input(shape=(1,))\n",
    "output_day = Embedding(31, 10, name='day_embedding')(input_day)\n",
    "output_day = Reshape(target_shape=(10,))(output_day)\n",
    "\n",
    "input_germanstate = Input(shape=(1,))\n",
    "output_germanstate = Embedding(12, 6, name='state_embedding')(input_germanstate)\n",
    "output_germanstate = Reshape(target_shape=(6,))(output_germanstate)\n",
    "\n",
    "input_model = [input_store, input_dow, input_promo,\n",
    "                input_year, input_month, input_day, input_germanstate]\n",
    "\n",
    "output_embeddings = [output_store, output_dow, output_promo,\n",
    "                      output_year, output_month, output_day, output_germanstate]\n",
    "\n",
    "output_model = Concatenate()(output_embeddings)\n",
    "output_model = Dense(1000, kernel_initializer=\"uniform\")(output_model)\n",
    "output_model = Activation('relu')(output_model)\n",
    "output_model = Dense(500, kernel_initializer=\"uniform\")(output_model)\n",
    "output_model = Activation('relu')(output_model)\n",
    "output_model = Dense(1)(output_model)\n",
    "output_model = Activation('sigmoid')(output_model)\n",
    "modelNN_emb = KerasModel(inputs=input_model, outputs=output_model)\n",
    "modelNN_emb.compile(loss='mean_absolute_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tqUNSxR8q3-B"
   },
   "outputs": [],
   "source": [
    "#we will rescale our y_train for the model\n",
    "#the reason for this is mentioned in the paper in the same section\n",
    "#your code here\n",
    "max_log_y = max(np.max(np.log(y_train)), np.max(np.log(y_val)))\n",
    "fitting_ytr = np.log(y_train) / max_log_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "c_A-7KpOMG2B"
   },
   "outputs": [],
   "source": [
    "fitting_yval = np.log(y_val) / max_log_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjtLWGheNUMP",
    "outputId": "de633a12-58dd-4eb2-e014-99022d49d703"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200000,), (200000,))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WcPVMLiF-1v"
   },
   "source": [
    "Now that we have built the model, we need to ensure that the input passed to the model is also in the same format - hence we will define a function split the features we are performing embeddings on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "o1ZSq3RetpUx"
   },
   "outputs": [],
   "source": [
    "#split the features\n",
    "def split_features(X):\n",
    "    X_list = []\n",
    "\n",
    "    store_index = X[..., [1]]\n",
    "    X_list.append(store_index)\n",
    "\n",
    "    day_of_week = X[..., [2]]\n",
    "    X_list.append(day_of_week)\n",
    "\n",
    "    promo = X[..., [3]]\n",
    "    X_list.append(promo)\n",
    "\n",
    "    year = X[..., [4]]\n",
    "    X_list.append(year)\n",
    "\n",
    "    month = X[..., [5]]\n",
    "    X_list.append(month)\n",
    "\n",
    "    day = X[..., [6]]\n",
    "    X_list.append(day)\n",
    "\n",
    "    State = X[..., [7]]\n",
    "    X_list.append(State)\n",
    "\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gn0ZRL6at7xg"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "    X_list = split_features(X)\n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJKlX4TdGNcn"
   },
   "source": [
    "Fit the model on the `preprocessing(X_train)` and on the fitting_y with 10 epochs and batch_size as 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BCE5qF0lrEmJ",
    "outputId": "41ceecea-808e-44c0-f152-c895c746f41d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 0.0248 - val_loss: 0.0426\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04263, saving model to best_model_weights.hdf5\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0096 - val_loss: 0.0437\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.04263\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0086 - val_loss: 0.0431\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.04263\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0080 - val_loss: 0.0435\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.04263\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0076 - val_loss: 0.0433\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.04263\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0073 - val_loss: 0.0433\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04263\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0071 - val_loss: 0.0436\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04263\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0070 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04263\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0068 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04263\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 0.0068 - val_loss: 0.0430\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0377828410>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelNN_emb.checkpointer = ModelCheckpoint(filepath=\"best_model_weights.hdf5\", verbose=1, save_best_only=True)\n",
    "modelNN_emb.fit(preprocessing(X_train), fitting_ytr, epochs = 10, batch_size = 128, validation_data= (preprocessing(X_train),fitting_yval), callbacks=[modelNN_emb.checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX4q6sj2qBTy"
   },
   "source": [
    "Now we will save the embeddings from the model defined above:\n",
    "check if save_embeddings is set to True first, if yes store the following:\n",
    "  1. store_embedding\n",
    "  2. dow_embedding\n",
    "  3. year_embedding\n",
    "  4. month_embedding\n",
    "  5. day_embedding\n",
    "  6. state_embedding\n",
    "\n",
    "\n",
    "Save this entire embeddings into the pickle file - **saved_embeddings_fname - embeddings.pickle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "o7QS4kLLqKrH"
   },
   "outputs": [],
   "source": [
    "if save_embeddings:\n",
    "    #your code here\n",
    "    store_embedding = modelNN_emb.get_layer('store_embedding').get_weights()[0]\n",
    "    dow_embedding = modelNN_emb.get_layer('dow_embedding').get_weights()[0]\n",
    "    year_embedding = modelNN_emb.get_layer('year_embedding').get_weights()[0]\n",
    "    month_embedding = modelNN_emb.get_layer('month_embedding').get_weights()[0]\n",
    "    day_embedding = modelNN_emb.get_layer('day_embedding').get_weights()[0]\n",
    "    german_states_embedding = modelNN_emb.get_layer('state_embedding').get_weights()[0]\n",
    "    with open(saved_embeddings_fname, 'wb') as f:\n",
    "        pickle.dump([store_embedding, dow_embedding, year_embedding,\n",
    "                     month_embedding, day_embedding, german_states_embedding], f, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5mmmJi_BCc"
   },
   "source": [
    "# You are done with Part 3!! \n",
    "Remember to save the embeddings.pickle in handy - this will be used in Part 4!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Hw2_Part3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
